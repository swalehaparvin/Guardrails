#üö¶ LLM Guardrails: Colab Notebooks

This repository provides Google Colab notebooks for experimenting with different guardrail frameworks used in securing and controlling Large Language Models (LLMs).
It covers hands-on tutorials, code samples, and comparison experiments across three widely discussed frameworks:
	‚Ä¢	NVIDIA NeMo Guardrails
	‚Ä¢	Llama Guard (Meta‚Äôs safety classifier)
	‚Ä¢	Guardrails AI
üõ†Ô∏è Frameworks Covered

1. NVIDIA NeMo Guardrails
	‚Ä¢	Rule-based + ML-driven framework for enforcing safety, security, and compliance.
	‚Ä¢	Supports flow definition files (Colang) for conversational control.
	‚Ä¢	Integration with multiple LLM providers.
	‚Ä¢	Example use cases: content filtering, data redaction, safety checks.

‚∏ª

2. Llama Guard
	‚Ä¢	Lightweight guardrail model released by Meta.
	‚Ä¢	Functions as a safety classifier that detects policy-violating generations.
	‚Ä¢	Can be used as a filter alongside other LLMs.
	‚Ä¢	Example use cases: toxicity detection, harmful instruction blocking.

‚∏ª

3. Guardrails AI
	‚Ä¢	Open-source Python library for specifying, validating, and enforcing constraints on LLM outputs.
	‚Ä¢	Schema-driven approach with pydantic-like validators.
	‚Ä¢	Strong focus on structured outputs (JSON/XML/YAML) and semantic validation.
	‚Ä¢	Example use cases: ensuring responses follow format, preventing hallucinations.


‚∏ª

Run in Google Colab

Each notebook can be opened and run directly in Google Colab:
	‚Ä¢	NVIDIA NeMo Guardrails Notebook
	‚Ä¢	Llama Guard Notebook
	‚Ä¢	Guardrails AI Notebook

‚∏ª
